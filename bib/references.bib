

@InProceedings{Sha_Confidence_MICCAI2024,
        author = { Sharma, Saurabh and Kumar, Atul and Chandra, Joydeep},
        title = { { Confidence Matters: Enhancing Medical Image Classification Through Uncertainty-Driven Contrastive Self-Distillation } },
        booktitle = {proceedings of Medical Image Computing and Computer Assisted Intervention -- MICCAI 2024},
        year = {2024},
		doi={https://doi.org/10.1007/978-3-031-72117-5_13},
        publisher = {Springer Nature Switzerland},
        volume = {LNCS 15010},
        month = {October},
        page = {133 -- 142},
	type = {Knowledge Distillation Methods, Supervised Learning, Image Classification}
}

@inproceedings{Xing2021CategoricalRC,
  title={Categorical Relation-Preserving Contrastive Knowledge Distillation for Medical Image Classification},
  author={Xiaohan Xing and Yuenan Hou and Han Li and Yixuan Yuan and Hongsheng Li and Max Q.‐H. Meng},
  booktitle={International Conference on Medical Image Computing and Computer-Assisted Intervention},
  year={2021},
  doi={https://doi.org/10.48550/arXiv.2107.03225}
  url={https://api.semanticscholar.org/CorpusID:235755366}
}

@article{AZAD2024103000,
title = {Advances in medical image analysis with vision Transformers: A comprehensive review},
journal = {Medical Image Analysis},
volume = {91},
pages = {103000},
year = {2024},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2023.103000},
url = {https://www.sciencedirect.com/science/article/pii/S1361841523002608},
author = {Reza Azad and Amirhossein Kazerouni and Moein Heidari and Ehsan Khodapanah Aghdam and Amirali Molaei and Yiwei Jia and Abin Jose and Rijo Roy and Dorit Merhof},
keywords = {Transformers, Medical image analysis, Vision transformers, Deep neural networks},
abstract = {The remarkable performance of the Transformer architecture in natural language processing has recently also triggered broad interest in Computer Vision. Among other merits, Transformers are witnessed as capable of learning long-range dependencies and spatial correlations, which is a clear advantage over convolutional neural networks (CNNs), which have been the de facto standard in Computer Vision problems so far. Thus, Transformers have become an integral part of modern medical image analysis. In this review, we provide an encyclopedic review of the applications of Transformers in medical imaging. Specifically, we present a systematic and thorough review of relevant recent Transformer literature for different medical image analysis tasks, including classification, segmentation, detection, registration, synthesis, and clinical report generation. For each of these applications, we investigate the novelty, strengths and weaknesses of the different proposed strategies and develop taxonomies highlighting key properties and contributions. Further, if applicable, we outline current benchmarks on different datasets. Finally, we summarize key challenges and discuss different future research directions. In addition, we have provided cited papers with their corresponding implementations in https://github.com/mindflow-institue/Awesome-Transformer.}
}

@InProceedings{10.1007/978-3-031-72117-5_52,
author={Chen, Aobo
and Li, Yangyi
and Qian, Wei
and Morse, Kathryn
and Miao, Chenglin
and Huai, Mengdi},
editor={Linguraru, Marius George
and Dou, Qi
and Feragen, Aasa
and Giannarou, Stamatia
and Glocker, Ben
and Lekadir, Karim
and Schnabel, Julia A.},
title={Modeling and Understanding Uncertainty in Medical Image Classification},
booktitle={Medical Image Computing and Computer Assisted Intervention -- MICCAI 2024},
year={2024},
doi={https://doi.org/10.1007/978-3-031-72117-5_52}
publisher={Springer Nature Switzerland},
address={Cham},
pages={557--567},
abstract={Medical image classification is an important task in many different medical applications. The past years have witnessed the success of Deep Neural Networks (DNNs) in medical image classification. However, traditional softmax outputs produced by DNNs fail to estimate uncertainty in medical image predictions. Contrasting with conventional uncertainty estimation approaches, conformal prediction (CP) stands out as a model-agnostic and distribution-free methodology that constructs statistically rigorous uncertainty sets for model predictions. However, existing exact full conformal methods involve retraining the underlying DNN model for each test instance with each possible label, demanding substantial computational resources. Additionally, existing works fail to uncover the root causes of medical prediction uncertainty, making it difficult for doctors to interpret the estimated uncertainties associated with medical diagnoses. To address these challenges, in this paper, we first propose an efficient approximate full CP method, which involves tracking the gradient updates contributed by these samples during training. Subsequently, we design an interpretation method that uses these updates to identify the top-k most influential training samples that significantly impact models' uncertainties. Extensive experiments on real-world medical image datasets are conducted to verify the effectiveness of the proposed methods.},
isbn={978-3-031-72117-5}
}

@InProceedings{10.1007/978-3-031-72120-5_42,
author={Chowdary, G. Jignesh
and Yin, Zhaozheng},
editor={Linguraru, Marius George
and Dou, Qi
and Feragen, Aasa
and Giannarou, Stamatia
and Glocker, Ben
and Lekadir, Karim
and Schnabel, Julia A.},
title={Med-Former: A Transformer Based Architecture for Medical Image Classification},
booktitle={Medical Image Computing and Computer Assisted Intervention -- MICCAI 2024},
year={2024},
doi={https://doi.org/10.1007/978-3-031-72120-5_42}
Keywords: {Medical Image Classification · Transformers · Computer Aided Diagnosis · Local-global Feature Extraction · Spatial Attention Fusion},
publisher={Springer Nature Switzerland},
address={Cham},
pages={448--457},
abstract={In recent years, transformer-based image classification methods have demonstrated remarkable effectiveness across various image classification tasks. However, their application to medical images presents challenges, especially in the feature extraction capability of the network. Additionally, these models often struggle with the efficient propagation of essential information throughout the network, hindering their performance in medical imaging tasks. To overcome these challenges, we introduce a novel framework comprising Local-Global Transformer module and Spatial Attention Fusion module, collectively referred to as Med-Former. These modules are specifically designed to enhance the feature extraction capability at both local and global levels and improve the propagation of vital information within the network. To evaluate the efficacy of our proposed Med-Former framework, we conducted experiments on three publicly available medical image datasets: NIH Chest X-ray14, DermaMNIST, and BloodMNIST. Our results demonstrate that Med-Former outperforms state-of-the-art approaches underscoring its superior generalization capability and effectiveness in medical image classification.},
isbn={978-3-031-72120-5}
}



@InProceedings{Sha_Confidence_MICCAI2024,
        author = { Sharma, Saurabh and Kumar, Atul and Chandra, Joydeep},
        title = { { Confidence Matters: Enhancing Medical Image Classification Through Uncertainty-Driven Contrastive Self-Distillation } },
        booktitle = {proceedings of Medical Image Computing and Computer Assisted Intervention -- MICCAI 2024},
        year = {2024},
		doi={https://doi.org/10.1007/978-3-031-72117-5_13},
        publisher = {Springer Nature Switzerland},
        volume = {LNCS 15010},
        month = {October},
        page = {133 -- 142},
		image = {paper1.png},
		type = {Knowledge Distillation Methods, Supervised Learning, Image Classification}
}

@inproceedings{Xing2021CategoricalRC,
  title={Categorical Relation-Preserving Contrastive Knowledge Distillation for Medical Image Classification},
  author={Xiaohan Xing and Yuenan Hou and Han Li and Yixuan Yuan and Hongsheng Li and Max Q.‐H. Meng},
  booktitle={International Conference on Medical Image Computing and Computer-Assisted Intervention},
  year={2021},
  doi={https://doi.org/10.48550/arXiv.2107.03225}
  url={https://api.semanticscholar.org/CorpusID:235755366},
  type = {Knowledge Distillation Methods, Supervised Learning, Image Classification}
}

@article{AZAD2024103000,
title = {Advances in medical image analysis with vision Transformers: A comprehensive review},
journal = {Medical Image Analysis},
volume = {91},
pages = {103000},
year = {2024},
issn = {1361-8415},
doi = {https://doi.org/10.1016/j.media.2023.103000},
url = {https://www.sciencedirect.com/science/article/pii/S1361841523002608},
author = {Reza Azad and Amirhossein Kazerouni and Moein Heidari and Ehsan Khodapanah Aghdam and Amirali Molaei and Yiwei Jia and Abin Jose and Rijo Roy and Dorit Merhof},
keywords = {Transformers, Medical image analysis, Vision transformers, Deep neural networks},
abstract = {The remarkable performance of the Transformer architecture in natural language processing has recently also triggered broad interest in Computer Vision. Among other merits, Transformers are witnessed as capable of learning long-range dependencies and spatial correlations, which is a clear advantage over convolutional neural networks (CNNs), which have been the de facto standard in Computer Vision problems so far. Thus, Transformers have become an integral part of modern medical image analysis. In this review, we provide an encyclopedic review of the applications of Transformers in medical imaging. Specifically, we present a systematic and thorough review of relevant recent Transformer literature for different medical image analysis tasks, including classification, segmentation, detection, registration, synthesis, and clinical report generation. For each of these applications, we investigate the novelty, strengths and weaknesses of the different proposed strategies and develop taxonomies highlighting key properties and contributions. Further, if applicable, we outline current benchmarks on different datasets. Finally, we summarize key challenges and discuss different future research directions. In addition, we have provided cited papers with their corresponding implementations in https://github.com/mindflow-institue/Awesome-Transformer.}
}

@InProceedings{10.1007/978-3-031-72117-5_52,
author={Chen, Aobo
and Li, Yangyi
and Qian, Wei
and Morse, Kathryn
and Miao, Chenglin
and Huai, Mengdi},
editor={Linguraru, Marius George
and Dou, Qi
and Feragen, Aasa
and Giannarou, Stamatia
and Glocker, Ben
and Lekadir, Karim
and Schnabel, Julia A.},
title={Modeling and Understanding Uncertainty in Medical Image Classification},
booktitle={Medical Image Computing and Computer Assisted Intervention -- MICCAI 2024},
year={2024},
doi={https://doi.org/10.1007/978-3-031-72117-5_52}
publisher={Springer Nature Switzerland},
address={Cham},
pages={557--567},
abstract={Medical image classification is an important task in many different medical applications. The past years have witnessed the success of Deep Neural Networks (DNNs) in medical image classification. However, traditional softmax outputs produced by DNNs fail to estimate uncertainty in medical image predictions. Contrasting with conventional uncertainty estimation approaches, conformal prediction (CP) stands out as a model-agnostic and distribution-free methodology that constructs statistically rigorous uncertainty sets for model predictions. However, existing exact full conformal methods involve retraining the underlying DNN model for each test instance with each possible label, demanding substantial computational resources. Additionally, existing works fail to uncover the root causes of medical prediction uncertainty, making it difficult for doctors to interpret the estimated uncertainties associated with medical diagnoses. To address these challenges, in this paper, we first propose an efficient approximate full CP method, which involves tracking the gradient updates contributed by these samples during training. Subsequently, we design an interpretation method that uses these updates to identify the top-k most influential training samples that significantly impact models' uncertainties. Extensive experiments on real-world medical image datasets are conducted to verify the effectiveness of the proposed methods.},
isbn={978-3-031-72117-5}
}

@InProceedings{10.1007/978-3-031-72120-5_42,
author={Chowdary, G. Jignesh
and Yin, Zhaozheng},
editor={Linguraru, Marius George
and Dou, Qi
and Feragen, Aasa
and Giannarou, Stamatia
and Glocker, Ben
and Lekadir, Karim
and Schnabel, Julia A.},
title={Med-Former: A Transformer Based Architecture for Medical Image Classification},
booktitle={Medical Image Computing and Computer Assisted Intervention -- MICCAI 2024},
year={2024},
doi={https://doi.org/10.1007/978-3-031-72120-5_42}
Keywords: {Medical Image Classification · Transformers · Computer Aided Diagnosis · Local-global Feature Extraction · Spatial Attention Fusion},
publisher={Springer Nature Switzerland},
address={Cham},
pages={448--457},
abstract={In recent years, transformer-based image classification methods have demonstrated remarkable effectiveness across various image classification tasks. However, their application to medical images presents challenges, especially in the feature extraction capability of the network. Additionally, these models often struggle with the efficient propagation of essential information throughout the network, hindering their performance in medical imaging tasks. To overcome these challenges, we introduce a novel framework comprising Local-Global Transformer module and Spatial Attention Fusion module, collectively referred to as Med-Former. These modules are specifically designed to enhance the feature extraction capability at both local and global levels and improve the propagation of vital information within the network. To evaluate the efficacy of our proposed Med-Former framework, we conducted experiments on three publicly available medical image datasets: NIH Chest X-ray14, DermaMNIST, and BloodMNIST. Our results demonstrate that Med-Former outperforms state-of-the-art approaches underscoring its superior generalization capability and effectiveness in medical image classification.},
isbn={978-3-031-72120-5}
}


@INPROCEEDINGS{10651469,
  author={Huang, Xinlei and Jiang, Ning and Tang, Jialiang},
  booktitle={2024 International Joint Conference on Neural Networks (IJCNN)}, 
  title={ClearKD: Clear Knowledge Distillation for Medical Image Classification}, 
  year={2024},
  volume={},
  number={},
  pages={1-8},
  keywords={Knowledge engineering;Degradation;Accuracy;Design automation;Interference;Prediction algorithms;Skin;skin lesions classification;brain tumor classification;model compression;knowledge distillation},
  doi={10.1109/IJCNN60899.2024.10651469}
}


@INPROCEEDINGS{9533719,
  author={Calderón-Ramírez, Saúl and Murillo-Hernández, Diego and Rojas-Salazar, Kevin and Calvo-Valverd, Luis-Alexander and Yang, Shengxiang and Moemeni, Armaghan and Elizondo, David and López-Rubio, Ezequiel and Molina-Cabello, Miguel A.},
  booktitle={2021 International Joint Conference on Neural Networks (IJCNN)}, 
  title={Improving Uncertainty Estimations for Mammogram Classification using Semi-Supervised Learning}, 
  year={2021},
  volume={},
  number={},
  pages={1-8},
  keywords={Deep learning;Training;Uncertainty;Computational modeling;Neural networks;Estimation;Semisupervised learning;Uncertainty Estimation;Breast Cancer;Mammogram;Semi-Supervised Deep Learning;MixMatch},
  doi={10.1109/IJCNN52387.2021.9533719}}
  
@INPROCEEDINGS{9412946,
  author={Calderon-Ramirez, Saul and Giri, Raghvendra and Yang, Shengxiang and Moemeni, Armaghan and Umaña, Mario and Elizondo, David and Torrents-Barrena, Jordina and Molina-Cabello, Miguel A.},
  booktitle={2020 25th International Conference on Pattern Recognition (ICPR)}, 
  title={Dealing with Scarce Labelled Data: Semi-supervised Deep Learning with Mix Match for Covid-19 Detection Using Chest X-ray Images}, 
  abstract = {Coronavirus (Covid-19) is spreading fast, infecting people through contact in various forms including droplets from sneezing and coughing. Therefore, the detection of infected subjects in an early, quick and cheap manner is urgent. Currently available tests are scarce and limited to people in danger of serious illness. The application of deep learning to chest X-ray images for Covid-19 detection is an attractive approach. However, this technology usually relies on the availability of large labelled datasets, a requirement hard to meet in the context of a virus outbreak. To overcome this challenge, a semi-supervised deep learning model using both labelled and unlabelled data is proposed. We developed and tested a semi-supervised deep learning framework based on the Mix Match architecture to classify chest X-rays into Covid-19, pneumonia and healthy cases. The presented approach was calibrated using two publicly available datasets. The results show an accuracy increase of around 15\% under low labelled / unlabelled data ratio. This indicates that our semi-supervised framework can help improve performance levels towards Covid-19 detection when the amount of high-quality labelled data is scarce. Also, we introduce a semi-supervised deep learning boost coefficient which is meant to ease the scalability of our approach and performance comparison.},
  conference = { 2020 25th International Conference on Pattern Recognition (ICPR 2020)},
  year={2021},
  volume={},
  number={},
  pages={5294-5301},
  keywords={Deep learning;COVID-19;Training;Solid modeling;Scalability;X-rays;Semisupervised learning;Semi-supervised Deep Learning;Mix Match;Chest X-Ray;Covid-19;Computer Aided Diagnosis},
  doi={10.1109/ICPR48806.2021.9412946}}

@misc { ,
	title = {Weakly Supervised Segmentation with Point Annotations for Histopathology Images via Contrast-Based Variational Model},
	abstract = {Image segmentation is a fundamental task in the field of imaging and vision. Supervised deep learning for segmentation has achieved unparalleled success when sufficient training data with annotated labels are available. However, annotation is known to be expensive to obtain, especially for histopathology images where the target regions are usually with high morphology variations and irregular shapes. Thus, weakly supervised learning with sparse annotations of points is promising to reduce the annotation workload. In this work, we propose a contrast-based variational model to generate segmentation results, which serve as reliable complementary supervision to train a deep segmentation model for histopathology images. The proposed method considers the common characteristics of target regions in histopathology images and can be trained in an end-to-end manner. It can generate more regionally consistent and smoother boundary segmentation, and is more robust to unlabeled ‘novel’ regions. Experiments on two different histology datasets demonstrate its effectiveness and efficiency in comparison to previous models. Code is available at: https://github.com/hrzhang1123/CVM\_WS\_Segmentation.},
	conference = {2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
	doi = {10.1109/cvpr52729.2023.01500},
	isbn = {979-8-3503-0130-4},
	pages = {15630-15640},
	publicationstatus = {Published},
	publisher = {Institute of Electrical and Electronics Engineers},
	url = {https://nottingham-repository.worktribe.com/output/25057462},
	volume = {2023-June},
	Keyword = {Image segmentation , Histopathology , Annotations , Shape , Supervised learning , Training data , Morphology , Medical and biological vision , cell microscopy},
	year = {2023},
	author = {Zhang, Hongrun and Burrows, Liam and Meng, Yanda and Sculthorpe, Declan and Mukherjee, Abhik and Coupland, Sarah E. and Chen, Ke and Zheng, Yalin}
}

@article{PACHETTI2024102949,
title = {A systematic review of few-shot learning in medical imaging},
journal = {Artificial Intelligence in Medicine},
volume = {156},
pages = {102949},
year = {2024},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2024.102949},
url = {https://www.sciencedirect.com/science/article/pii/S093336572400191X},
author = {Eva Pachetti and Sara Colantonio},
keywords = {Few-shot learning, Medical imaging, Systematic review},
abstract = {The lack of annotated medical images limits the performance of deep learning models, which usually need large-scale labelled datasets. Few-shot learning techniques can reduce data scarcity issues and enhance medical image analysis speed and robustness. This systematic review gives a comprehensive overview of few-shot learning methods for medical image analysis, aiming to establish a standard methodological pipeline for future research reference. With a particular emphasis on the role of meta-learning, we analysed 80 relevant articles published from 2018 to 2023, conducting a risk of bias assessment and extracting relevant information, especially regarding the employed learning techniques. From this, we delineated a comprehensive methodological pipeline shared among all studies. In addition, we performed a statistical analysis of the studies’ results concerning the clinical task and the meta-learning method employed while also presenting supplemental information such as imaging modalities and model robustness evaluation techniques. We discussed the findings of our analysis, providing a deep insight into the limitations of the state-of-the-art methods and the most promising approaches. Drawing on our investigation, we yielded recommendations on potential future research directions aiming to bridge the gap between research and clinical practice.}
}
